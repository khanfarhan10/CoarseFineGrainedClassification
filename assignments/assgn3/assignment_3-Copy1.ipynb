{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "1. You have to use only this notebook for all your code.\n",
    "2. All the results and plots should be mentioned in this notebook.\n",
    "3. For final submission, submit this notebook along with the report ( usual 2-4 pages, latex typeset, which includes the challenges faces and details of additional steps, if any)\n",
    "4. Marking scheme\n",
    "    -  **60%**: Your code should be able to detect bounding boxes using resnet 18, correct data loading and preprocessing. Plot any 5 correct and 5 incorrect sample detections from the test set in this notebook for both the approached (1 layer and 2 layer detection), so total of 20 plots.\n",
    "    -  **20%**: Use two layers (multi-scale feature maps) to detect objects independently as in SSD (https://arxiv.org/abs/1512.02325).  In this method, 1st detection will be through the last layer of Resnet18 and the 2nd detection could be through any layer before the last layer. SSD uses lower resolution layers to detect larger scale objects. \n",
    "    -  **20%**: Implement Non-maximum suppression (NMS) (should not be imported from any library) on the candidate bounding boxes.\n",
    "    \n",
    "5. Report AP for each of the three class and mAP score for the complete test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "from shutil import copyfile\n",
    "from torchvision import datasets, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "# Import other modules if required\n",
    "# Can use other libraries as well\n",
    "\n",
    "resnet_input = 224 #size of resnet18 input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your hyper-parameters using validation data\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate =  0.005\n",
    "# hyp_momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build the data\n",
    "Use the following links to locally download the data:\n",
    "<br/>Training and validation:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "<br/>Testing data:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "<br/>The dataset consists of images from 20 classes, with detection annotations included. The JPEGImages folder houses the images, and the Annotations folder has the object-wise labels for the objects in one xml file per image. You have to extract the object information, i.e. the [xmin, ymin] (the top left x,y co-ordinates) and the [xmax, ymax] (the bottom right x,y co-ordinates) of only the objects belonging to the three classes(aeroplane, bottle, chair). For parsing the xml file, you can import xml.etree.ElementTree for you. <br/>\n",
    "<br/> Organize the data as follows:\n",
    "<br/> For every image in the dataset, extract/crop the object patch from the image one by one using their respective co-ordinates:[xmin, ymin, xmax, ymax], resize the image to resnet_input, and store it with its class label information. Do the same for training/validation and test datasets. <br/>\n",
    "##### Important\n",
    "You also have to collect data for an extra background class which stands for the class of an object which is not a part of any of the 20 classes. For this, you can crop and resize any random patches from an image. A good idea is to extract patches that have low \"intersection over union\" with any object present in the image frame from the 20 Pascal VOC classes. The number of background images should be roughly around those of other class objects' images. Hence the total classes turn out to be four. This is important for applying the sliding window method later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('__background__',\n",
    "           'aeroplane',\n",
    "           'bottle','chair'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "def intersection_over_union(orig_boxes,bg_boxes,img,file):\n",
    "    global k\n",
    "    for bg_box in bg_boxes:\n",
    "        flag = 0\n",
    "        for orig_box in orig_boxes:\n",
    "            boxA = orig_box\n",
    "            boxB = bg_box\n",
    "            xA = max(boxA[0], boxB[0])\n",
    "            yA = max(boxA[1], boxB[1])\n",
    "            xB = min(boxA[2], boxB[2])\n",
    "            yB = min(boxA[3], boxB[3])\n",
    " \n",
    "            # compute the area of intersection rectangle\n",
    "            interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    " \n",
    "            # compute the area of both the prediction and ground-truth\n",
    "            # rectangles\n",
    "            boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "            boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    " \n",
    "            # compute the intersection over union by taking the intersection\n",
    "            # area and dividing it by the sum of prediction + ground-truth\n",
    "            # areas - the interesection area\n",
    "            iou = float(interArea) / float(boxAArea + boxBArea - interArea)\n",
    "            if file.split('.')[0]=='003644':\n",
    "                    print(orig_box,bg_box,interArea,iou)\n",
    "            if interArea > 0:\n",
    "                flag=1\n",
    "                break\n",
    "#             if interArea==0 and k<700:\n",
    "        if not flag and k < 700:\n",
    "            cropped_img = img.crop(bg_box)\n",
    "            new_image_path = \"data2/processed_data/{}/{}_{}\".format('__background__', str(k), file.split('.')[0] + \".jpg\")\n",
    "            cropped_img.save(new_image_path)\n",
    "            k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset():\n",
    "    c=0\n",
    "    found_classes = {'__background__': 0}\n",
    "    processed_dir = 'data2/processed_data'\n",
    "    if os.path.exists(processed_dir):\n",
    "        shutil.rmtree(processed_dir)\n",
    "    os.makedirs(processed_dir)\n",
    "    data_path = \"data2/VOCdevkit/Annotations/\"\n",
    "    images_path = \"data2/VOCdevkit/JPEGImages/\"\n",
    "    for i in classes:\n",
    "        directory = \"data2/processed_data/{}\".format(i)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    count = 0\n",
    "    for file in os.listdir(data_path):\n",
    "        flag = 0\n",
    "        f = open(data_path + file, \"r\").read()\n",
    "        xml = ET.fromstring(f)\n",
    "        objects = xml.findall('./object')\n",
    "        boxes = []\n",
    "        for obj in objects:\n",
    "            img_class = obj.find('name').text\n",
    "            if img_class in classes:\n",
    "                c+=1\n",
    "                if found_classes.get(img_class, None) is not None:\n",
    "                    found_classes[img_class] += 1\n",
    "                else:\n",
    "                    found_classes[img_class] = 1\n",
    "                box = obj.find('bndbox')\n",
    "                xmin = int(box.find('xmin').text)\n",
    "                ymin = int(box.find('ymin').text)\n",
    "                ymax = int(box.find('ymax').text)\n",
    "                xmax = int(box.find('xmax').text)\n",
    "                image_path = images_path + file.split('.')[0] + \".jpg\"\n",
    "                img = Image.open(image_path)\n",
    "                area = (xmin, ymin, xmax, ymax)\n",
    "                cropped_img = img.crop(area)\n",
    "                new_image_path = \"data2/processed_data/{}/{}_{}\".format(img_class, str(c), file.split('.')[0] + \".jpg\")\n",
    "                cropped_img.save(new_image_path)\n",
    "                flag = 1\n",
    "            box = obj.find('bndbox')\n",
    "            xmin = int(box.find('xmin').text)\n",
    "            ymin = int(box.find('ymin').text)\n",
    "            ymax = int(box.find('ymax').text)\n",
    "            xmax = int(box.find('xmax').text)\n",
    "            coordinates = [xmin, ymin, xmax, ymax]\n",
    "            boxes.append(coordinates)\n",
    "        image_path = images_path + file.split('.')[0] + \".jpg\"\n",
    "        img = Image.open(image_path)\n",
    "        width, height = img.size\n",
    "        if width<224 or height<224:\n",
    "            continue\n",
    "        bg = []\n",
    "        coordinates = [0,0,224,224]\n",
    "        bg.append(coordinates)\n",
    "        coordinates = [width-224,0,width,224]\n",
    "        bg.append(coordinates)\n",
    "        coordinates = [0,height-224,224,height]\n",
    "        bg.append(coordinates)\n",
    "        coordinates = [width-224,height-224,width,height]\n",
    "        bg.append(coordinates)\n",
    "        intersection_over_union(boxes,bg,img,file)\n",
    "#         if not flag and count < 600:\n",
    "#             image_path = images_path + file.split('.')[0] + \".jpg\"\n",
    "# #                 img = Image.open(image_path)\n",
    "# #                 area = (xmin, ymin, xmax, ymax)\n",
    "# #                 cropped_img = img.crop(area)\n",
    "#             new_image_path = \"data2/processed_data/{}/{}_{}\".format(classes[0], str(c), file.split('.')[0] + \".jpg\")\n",
    "# #                 cropped_img.save(new_image_path)\n",
    "#             copyfile(image_path, new_image_path)\n",
    "#             count+=1\n",
    "    found_classes['__background__'] = k\n",
    "    print(found_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[291, 104, 461, 281] [0, 0, 224, 224] 0 0.0\n",
      "[236, 101, 331, 194] [0, 0, 224, 224] 0 0.0\n",
      "[86, 110, 231, 281] [0, 0, 224, 224] 15985 0.26752242602758064\n",
      "[291, 104, 461, 281] [276, 0, 500, 224] 20691 0.34272510435301135\n",
      "[291, 104, 461, 281] [0, 57, 224, 281] 0 0.0\n",
      "[236, 101, 331, 194] [0, 57, 224, 281] 0 0.0\n",
      "[86, 110, 231, 281] [0, 57, 224, 281] 23908 0.46128615254008376\n",
      "[291, 104, 461, 281] [276, 57, 500, 281] 30438 0.6012444444444445\n",
      "{'__background__': 700, 'chair': 1432, 'aeroplane': 331, 'bottle': 634}\n"
     ]
    }
   ],
   "source": [
    "build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3097\n",
      "['__background__', 'aeroplane', 'bottle', 'chair']\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data2/processed_data/'\n",
    "def load_split_train_test(datadir, valid_size = .05):\n",
    "    train_transforms = transforms.Compose([transforms.Resize([resnet_input,resnet_input]),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       ])\n",
    "    test_transforms = transforms.Compose([transforms.Resize([resnet_input,resnet_input]),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      ])\n",
    "    train_data = datasets.ImageFolder(datadir,       \n",
    "                    transform=train_transforms)\n",
    "    test_data = datasets.ImageFolder(datadir,\n",
    "                    transform=test_transforms)\n",
    "    num_train = len(train_data)\n",
    "    print(num_train)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    from torch.utils.data.sampler import SubsetRandomSampler\n",
    "    train_idx, test_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    trainloader = torch.utils.data.DataLoader(train_data,\n",
    "                   sampler=train_sampler, batch_size=batch_size)\n",
    "    testloader = torch.utils.data.DataLoader(test_data,\n",
    "                   sampler=test_sampler, batch_size=batch_size)\n",
    "    return trainloader, testloader\n",
    "trainloader, testloader = load_split_train_test(data_dir, .1)\n",
    "# print(trainloader.dataset.)\n",
    "print(trainloader.dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class voc_dataset(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "#     def __init__(self, root_dir, train, transform=None):\n",
    "#         # Begin\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         # Begin\n",
    "        \n",
    "#     def __getitem__(self, idx):\n",
    "#        # Begin\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the netwok\n",
    "<br/>You can train the network on the created dataset. This will yield a classification network on the 4 classes of the VOC dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# composed_transform = transforms.Compose([transforms.Scale((resnet_input,resnet_input)),\n",
    "#                                          transforms.ToTensor(),\n",
    "#                                          transforms.RandomHorizontalFlip()])\n",
    "# train_dataset = voc_dataset(root_dir='', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "# test_dataset = voc_dataset(root_dir='', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "Use the pre-trained network to fine-tune the network in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, 4)\n",
    "\n",
    "# Add code for using CUDA here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# Update if any errors occur\n",
    "# optimizer = optim.SGD(resnet18.parameters(), learning_rate, hyp_momentum)\n",
    "\n",
    "# criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Layer Detection\n",
    "def train():\n",
    "#     epochs = 3\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    print_every = 20\n",
    "    train_losses, test_losses = [], []\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in trainloader:\n",
    "            steps += 1\n",
    "            inputs, labels = inputs.to(device),labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logps = model.forward(inputs)\n",
    "            loss = criterion(logps, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "                test_loss = 0\n",
    "                accuracy = 0\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in testloader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        logps = model.forward(inputs)\n",
    "                        batch_loss = criterion(logps, labels)\n",
    "                        test_loss += batch_loss.item()\n",
    "                        ps = torch.exp(logps)\n",
    "                        top_p, top_class = ps.topk(1, dim=1)\n",
    "                        equals = top_class == labels.view(*top_class.shape)\n",
    "                        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "                train_losses.append(running_loss/len(trainloader))\n",
    "                test_losses.append(test_loss/len(testloader))                    \n",
    "                print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                      f\"Train loss : {running_loss/print_every:.3f}.. \"\n",
    "                      f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "                      f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
    "                acc = accuracy/len(testloader)\n",
    "                if acc >= 0.99:\n",
    "                    torch.save(model, 'models/resnet18new.pth')\n",
    "                running_loss = 0\n",
    "                model.train()\n",
    "    torch.save(model, 'models/resnet18new.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10.. Train loss : 0.864.. Test loss: 0.421.. Test accuracy: 0.880\n",
      "Epoch 1/10.. Train loss : 0.388.. Test loss: 0.270.. Test accuracy: 0.912\n",
      "Epoch 2/10.. Train loss : 0.293.. Test loss: 0.413.. Test accuracy: 0.844\n",
      "Epoch 2/10.. Train loss : 0.260.. Test loss: 0.227.. Test accuracy: 0.922\n",
      "Epoch 3/10.. Train loss : 0.239.. Test loss: 0.235.. Test accuracy: 0.916\n",
      "Epoch 3/10.. Train loss : 0.185.. Test loss: 0.239.. Test accuracy: 0.909\n",
      "Epoch 4/10.. Train loss : 0.203.. Test loss: 0.216.. Test accuracy: 0.929\n",
      "Epoch 4/10.. Train loss : 0.183.. Test loss: 0.207.. Test accuracy: 0.921\n",
      "Epoch 5/10.. Train loss : 0.185.. Test loss: 0.229.. Test accuracy: 0.932\n",
      "Epoch 5/10.. Train loss : 0.175.. Test loss: 0.220.. Test accuracy: 0.927\n",
      "Epoch 5/10.. Train loss : 0.172.. Test loss: 0.188.. Test accuracy: 0.936\n",
      "Epoch 6/10.. Train loss : 0.157.. Test loss: 0.196.. Test accuracy: 0.926\n",
      "Epoch 6/10.. Train loss : 0.215.. Test loss: 0.260.. Test accuracy: 0.912\n",
      "Epoch 7/10.. Train loss : 0.146.. Test loss: 0.185.. Test accuracy: 0.932\n",
      "Epoch 7/10.. Train loss : 0.140.. Test loss: 0.219.. Test accuracy: 0.914\n",
      "Epoch 8/10.. Train loss : 0.139.. Test loss: 0.200.. Test accuracy: 0.935\n",
      "Epoch 8/10.. Train loss : 0.156.. Test loss: 0.226.. Test accuracy: 0.918\n",
      "Epoch 9/10.. Train loss : 0.154.. Test loss: 0.224.. Test accuracy: 0.924\n",
      "Epoch 9/10.. Train loss : 0.159.. Test loss: 0.306.. Test accuracy: 0.909\n",
      "Epoch 10/10.. Train loss : 0.106.. Test loss: 0.277.. Test accuracy: 0.918\n",
      "Epoch 10/10.. Train loss : 0.136.. Test loss: 0.198.. Test accuracy: 0.924\n",
      "Epoch 10/10.. Train loss : 0.113.. Test loss: 0.188.. Test accuracy: 0.923\n",
      "CPU times: user 9min 33s, sys: 41.1 s, total: 10min 15s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two Layer Detection (SSD)\n",
    "def train():\n",
    "    \n",
    "    # Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Accuracy Calculation\n",
    "For applying detection, use a slding window method to test the above trained trained network on the detection task:<br/>\n",
    "Take some windows of varying size and aspect ratios and slide it through the test image (considering some stride of pixels) from left to right, and top to bottom, detect the class scores for each of the window, and keep only those which are above a certain threshold value. There is a similar approach used in the paper -Faster RCNN by Ross Girshick, where he uses three diferent scales/sizes and three different aspect ratios, making a total of nine windows per pixel to slide. You need to write the code and use it in testing code to find the predicted boxes and their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window():\n",
    "    # Begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply non_maximum_supression to reduce the number of boxes. You are free to choose the threshold value for non maximum supression, but choose wisely [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(boxA,boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "    iou_ = float(interArea) / float(boxAArea + boxBArea - interArea)\n",
    "    return iou_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_maximum_supression(boxes,threshold = 0.3):\n",
    "    boxes_dict = {}\n",
    "    for box in boxes:\n",
    "        if box[1] in boxes_dict:\n",
    "            boxes_dict[box[1]].append(box)\n",
    "        else:\n",
    "            boxes_dict[box[1]] = [box]\n",
    "    bounding_box = []\n",
    "    for cls,box in boxes_dict:\n",
    "        max_score = 0\n",
    "        for box1 in box:\n",
    "            for box2 in box:\n",
    "                if iou(box1[0],box2[0])>threshold:\n",
    "                    if max_score<max(box1[2],box2[2]):\n",
    "                        max_score = max(box1[2],box2[2])\n",
    "                        if box1[2]>box2[2]:\n",
    "                            best_box = box1\n",
    "                        else:\n",
    "                            best_box = box2\n",
    "        bounding_box.append(best_box)\n",
    "    return bounding_box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Layer Detection\n",
    "def test(resnet18):\n",
    "    # Write loops for testing the model on the test set\n",
    "    # Also print out the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time test(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two Layer Detection\n",
    "def test(resnet18):\n",
    "    # Write loops for testing the model on the test set\n",
    "    # Also print out the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time test(resnet18)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
